{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "846fb777-2fd4-4795-97d0-430d088dc1e1",
   "metadata": {},
   "source": [
    "# 各種統一実験\n",
    "\n",
    "## optimizer\n",
    "- Adam\n",
    "\n",
    "## learning rate\n",
    "- 0.01\n",
    "- 0.0001\n",
    "\n",
    "## aggregator\n",
    "- sum\n",
    "- mean\n",
    "\n",
    "## batch size\n",
    "- 32\n",
    "\n",
    "## criterion\n",
    "- Cross entoropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f88a44d0-e0b0-4ce4-9d83-40e05bc212b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../codes\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from functools import partial\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data.dataset import Subset\n",
    "\n",
    "from optuna_cv_utils import cv_train, cv_test, make_datasets\n",
    "from set_data_folder import make_train_data\n",
    "\n",
    "from model import GCN\n",
    "from DGCNN import DGCNN_Model\n",
    "from GIN import GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4af8183-4c57-4260-b1ee-364847f4fc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "369e807c-af31-4c55-81b7-3c7be6549023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(data_list, num_epoch, model_name, adam_lr, pool=\"mean\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 交差検証\n",
    "    fold = KFold(\n",
    "        n_splits=10, shuffle=True, random_state=0\n",
    "    )\n",
    "\n",
    "    valid_accs = []\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(data_list)):\n",
    "        # set model\n",
    "        if model_name ==\"GCN\":\n",
    "            model = GCN(hidden_channels=64,\n",
    "                        num_classes=4, num_node_feature=1, pooling=pool).to(device)\n",
    "        elif model_name == \"DGCNN\":\n",
    "            model = DGCNN_Model(num_features=1, num_classes=4).to(device)\n",
    "        elif model_name == \"GIN\":\n",
    "            model = GIN(dim_features=1, dim_target=4, \n",
    "                        config={\"aggregation\": pool, \"dropout\": .5, \"hidden_units\": [32, 32, 32, 32],\"train_eps\": True}).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=adam_lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # split data\n",
    "        train_loader = DataLoader(\n",
    "            Subset(data_list, train_idx),\n",
    "            shuffle=True,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            Subset(data_list, valid_idx),\n",
    "            shuffle=False,\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        for epoch_idx in range(num_epoch):\n",
    "            # train\n",
    "            cv_train(model, train_loader, device, criterion, optimizer, model_name)\n",
    "            # valid\n",
    "            valid_acc = cv_test(model, valid_loader, device, model_name)\n",
    "\n",
    "        valid_accs.append(valid_acc)\n",
    "\n",
    "    return valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65659479-3447-42fc-9980-62b636cd3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:19<00:00, 250.27it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 267.69it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 255.97it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 267.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "0.3\n",
      "0.32\n",
      "0.33\n",
      "0.25\n",
      "0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 522.33it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 521.35it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 524.89it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 510.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n",
      "0.88\n",
      "0.86\n",
      "0.94\n",
      "0.52\n",
      "0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:15<00:00, 315.83it/s]\n",
      "100%|██████████| 5000/5000 [00:15<00:00, 326.66it/s]\n",
      "100%|██████████| 5000/5000 [00:15<00:00, 314.14it/s]\n",
      "100%|██████████| 5000/5000 [00:16<00:00, 302.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95\n",
      "0.48\n",
      "0.57\n",
      "0.81\n",
      "0.32\n",
      "0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:21<00:00, 229.88it/s]\n",
      "100%|██████████| 5000/5000 [00:20<00:00, 245.48it/s]\n",
      "100%|██████████| 5000/5000 [00:21<00:00, 229.49it/s]\n",
      "100%|██████████| 5000/5000 [00:21<00:00, 233.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n",
      "0.26\n",
      "0.72\n",
      "0.9\n",
      "0.25\n",
      "0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:26<00:00, 186.65it/s]\n",
      "100%|██████████| 5000/5000 [00:24<00:00, 201.71it/s]\n",
      "100%|██████████| 5000/5000 [00:26<00:00, 187.21it/s]\n",
      "100%|██████████| 5000/5000 [00:26<00:00, 185.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.25\n",
      "0.75\n",
      "0.76\n",
      "0.25\n",
      "0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:31<00:00, 156.51it/s]\n",
      "100%|██████████| 5000/5000 [00:28<00:00, 172.85it/s]\n",
      "100%|██████████| 5000/5000 [00:31<00:00, 157.69it/s]\n",
      "100%|██████████| 5000/5000 [00:33<00:00, 151.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36\n",
      "0.25\n",
      "0.47\n",
      "0.54\n",
      "0.25\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "m_list = [2, 4, 6, 8, 10]\n",
    "p_list = {\"100\": [0.04, 0.08, 0.11, 0.15, 0.19], \"1000\":[0.004, 0.008, 0.012, 0.016, 0.02]}\n",
    "step_list = {\"100\": [200, 384, 564, 736, 900], \"1000\":[1996, 3984, 5964, 7936, 9900]}\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"node\", \"p_s\", \"aggre\", \"adam_lr\", \"ave_acc\", \"std_acc\"])\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "# poisson dataset\n",
    "data_list = make_datasets()\n",
    "for model_name in [\"GCN\", \"DGCNN\"]:\n",
    "    for adam_lr in [0.01, 0.0001]:\n",
    "        if model_name == \"GCN\":\n",
    "            for p in [\"mean\", \"sum\"]:\n",
    "                valid_accs = CV(data_list, 10, model_name, adam_lr, pool=p)\n",
    "\n",
    "                info  = {\"model\": model_name, \"node\": \"100\", \"p_s\": f\"poisson\", \"aggre\": p,\n",
    "                 \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "                }\n",
    "\n",
    "                df = df.append(pd.Series(info, name=cnt))\n",
    "                cnt += 1\n",
    "                \n",
    "                print(round(np.average(valid_accs),2))\n",
    "        else:\n",
    "            valid_accs = CV(data_list, 10, model_name, adam_lr)\n",
    "\n",
    "            info  = {\"model\": model_name, \"node\": \"100\", \"p_s\": f\"poisson\", \"aggre\": \"None\",\n",
    "             \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "            }\n",
    "\n",
    "            df = df.append(pd.Series(info, name=cnt))\n",
    "            cnt += 1\n",
    "            print(round(np.average(valid_accs),2))\n",
    "\n",
    "# subset dataset\n",
    "for node in [\"100\"]:\n",
    "    for m, p, step in zip(m_list, p_list[node], step_list[node]):\n",
    "        # make train data folder\n",
    "        p_s = [\n",
    "            {\"kind\": \"barabasi\", \"node\": [node], \"p\": [str(m)]},\n",
    "            {\"kind\": \"noGrowth\", \"node\": [node], \"p\": [str(step)]},\n",
    "            {\"kind\": \"noAttach\", \"node\": [node], \"p\": [str(m)]},\n",
    "            {\"kind\": \"random\", \"node\": [node], \"p\": [str(p)]}\n",
    "        ]\n",
    "\n",
    "        # train data folderの作成\n",
    "        make_train_data(p_s, \"../train_data/net\").copy_data()\n",
    "        # dataset の作成\n",
    "        data_list = make_datasets()\n",
    "        \n",
    "        for model_name in [\"GCN\", \"DGCNN\"]:\n",
    "            for adam_lr in [0.01, 0.0001]:\n",
    "                if model_name == \"GCN\":\n",
    "                    for p in [\"mean\", \"sum\"]:\n",
    "                        valid_accs = CV(data_list, 10, model_name, adam_lr, pool=p)\n",
    "\n",
    "                        info  = {\"model\": model_name, \"node\": node, \"p_s\": f\"{m}_{p}_{step}\", \"aggre\": p,\n",
    "                         \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "                        }\n",
    "\n",
    "                        df = df.append(pd.Series(info, name=cnt))\n",
    "                        cnt += 1\n",
    "                        print(round(np.average(valid_accs),2))\n",
    "                else:\n",
    "                    valid_accs = CV(data_list, 10, model_name, adam_lr)\n",
    "\n",
    "                    info  = {\"model\": model_name, \"node\": node, \"p_s\": f\"{m}_{p}_{step}\", \"aggre\": \"None\",\n",
    "                     \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "                    }\n",
    "\n",
    "                    df = df.append(pd.Series(info, name=cnt))\n",
    "                    cnt += 1\n",
    "                    print(round(np.average(valid_accs),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2095dbdf-b3d7-4943-8d2d-f8cfb99f8949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"paper_result/re_result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1324d-dde4-4c61-b354-3bc917c94e7e",
   "metadata": {},
   "source": [
    "## GIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "060b2c33-9cfc-4b1c-a24e-cfa7d2719729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 513.37it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 518.09it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 516.64it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 509.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.97\n",
      "0.99\n",
      "0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 504.03it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 503.16it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 503.20it/s]\n",
      "100%|██████████| 5000/5000 [00:09<00:00, 507.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.98\n",
      "0.99\n",
      "0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:15<00:00, 317.34it/s]\n",
      "100%|██████████| 5000/5000 [00:15<00:00, 329.96it/s]\n",
      "100%|██████████| 5000/5000 [00:15<00:00, 317.77it/s]\n",
      "100%|██████████| 5000/5000 [00:16<00:00, 309.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "0.99\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:20<00:00, 238.61it/s]\n",
      "100%|██████████| 5000/5000 [00:20<00:00, 247.52it/s]\n",
      "100%|██████████| 5000/5000 [00:21<00:00, 236.23it/s]\n",
      "100%|██████████| 5000/5000 [00:20<00:00, 240.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:26<00:00, 190.06it/s]\n",
      "100%|██████████| 5000/5000 [00:24<00:00, 206.25it/s]\n",
      "100%|██████████| 5000/5000 [00:26<00:00, 190.75it/s]\n",
      "100%|██████████| 5000/5000 [00:26<00:00, 188.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.99\n",
      "0.99\n",
      "0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:31<00:00, 156.57it/s]\n",
      "100%|██████████| 5000/5000 [00:29<00:00, 172.22it/s]\n",
      "100%|██████████| 5000/5000 [00:31<00:00, 157.07it/s]\n",
      "100%|██████████| 5000/5000 [00:33<00:00, 150.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "m_list = [2, 4, 6, 8, 10]\n",
    "p_list = {\"100\": [0.04, 0.08, 0.11, 0.15, 0.19], \"1000\":[0.004, 0.008, 0.012, 0.016, 0.02]}\n",
    "step_list = {\"100\": [200, 384, 564, 736, 900], \"1000\":[1996, 3984, 5964, 7936, 9900]}\n",
    "\n",
    "df = pd.DataFrame(columns=[\"model\", \"node\", \"p_s\", \"aggre\", \"adam_lr\", \"ave_acc\", \"std_acc\"])\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "model_name = \"GIN\"\n",
    "# poisson dataset\n",
    "data_list = make_datasets()\n",
    "for adam_lr in [0.01, 0.0001]:\n",
    "    for p in [\"mean\", \"sum\"]:\n",
    "        valid_accs = CV(data_list, 10, model_name, adam_lr, pool=p)\n",
    "\n",
    "        info  = {\"model\": model_name, \"node\": \"100\", \"p_s\": f\"poisson\", \"aggre\": p,\n",
    "         \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "        }\n",
    "\n",
    "        df = df.append(pd.Series(info, name=cnt))\n",
    "        cnt += 1\n",
    "\n",
    "        print(round(np.average(valid_accs),2))\n",
    "    \n",
    "\n",
    "# subset dataset\n",
    "node = \"100\"\n",
    "for m, p, step in zip(m_list, p_list[node], step_list[node]):\n",
    "    # make train data folder\n",
    "    p_s = [\n",
    "        {\"kind\": \"barabasi\", \"node\": [node], \"p\": [str(m)]},\n",
    "        {\"kind\": \"noGrowth\", \"node\": [node], \"p\": [str(step)]},\n",
    "        {\"kind\": \"noAttach\", \"node\": [node], \"p\": [str(m)]},\n",
    "        {\"kind\": \"random\", \"node\": [node], \"p\": [str(p)]}\n",
    "    ]\n",
    "\n",
    "    # train data folderの作成\n",
    "    make_train_data(p_s, \"../train_data/net\").copy_data()\n",
    "    # dataset の作成\n",
    "    data_list = make_datasets()\n",
    "\n",
    "    for adam_lr in [0.01, 0.0001]:\n",
    "        for p in [\"mean\", \"sum\"]:\n",
    "            valid_accs = CV(data_list, 10, model_name, adam_lr, pool=p)\n",
    "\n",
    "            info  = {\"model\": model_name, \"node\": node, \"p_s\": f\"{m}_{p}_{step}\", \"aggre\": p,\n",
    "             \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "            }\n",
    "\n",
    "            df = df.append(pd.Series(info, name=cnt))\n",
    "            cnt += 1\n",
    "            print(round(np.average(valid_accs),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd931435-bb31-40e6-9ef9-7254ee4fbfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"paper_result/re_CV_GIN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de6dfd9c-bcd8-46cd-898e-207234ed72be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:18<00:00, 266.20it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 250.55it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 250.84it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 269.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.97\n",
      "0.99\n",
      "0.99\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"model\", \"node\", \"p_s\", \"aggre\", \"adam_lr\", \"ave_acc\", \"std_acc\"])\n",
    "\n",
    "cnt = 0\n",
    "model_name = \"GIN\"\n",
    "# poisson dataset\n",
    "data_list = make_datasets()\n",
    "for adam_lr in [0.01, 0.0001]:\n",
    "    for p in [\"mean\", \"sum\"]:\n",
    "        valid_accs = CV(data_list, 10, model_name, adam_lr, pool=p)\n",
    "\n",
    "        info  = {\"model\": model_name, \"node\": \"100\", \"p_s\": f\"poisson\", \"aggre\": p,\n",
    "         \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "        }\n",
    "\n",
    "        df = df.append(pd.Series(info, name=cnt))\n",
    "        cnt += 1\n",
    "\n",
    "        print(round(np.average(valid_accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f170a-9166-4c07-badf-8c1b68097740",
   "metadata": {},
   "source": [
    "# benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a0bfc3-626b-4ff0-857f-f45b1c468b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from model import GCN\n",
    "from DGCNN import DGCNN_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa89cb23-2144-40f1-b173-3c0e6812d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "def make_x(data):\n",
    "    G = to_networkx(data)\n",
    "    new_x = torch.tensor(\n",
    "        [[i] for i in list(dict(nx.degree(G)).values())],\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8348818-a0ec-4932-8e41-7a7183de2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(data_list, num_epoch, num_class, num_feature,model_name, adam_lr, pool=\"mean\"):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 交差検証\n",
    "    fold = KFold(\n",
    "        n_splits=10, shuffle=True, random_state=0\n",
    "    )\n",
    "\n",
    "    valid_accs = []\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(data_list)):\n",
    "        # set model\n",
    "        if model_name ==\"GCN\":\n",
    "            model = GCN(hidden_channels=64,\n",
    "                        num_classes=num_class, num_node_feature=num_feature, pooling=pool).to(device)\n",
    "        elif model_name == \"DGCNN\":\n",
    "            model = DGCNN_Model(num_features=num_feature, num_classes=num_class).to(device)\n",
    "            \n",
    "        optimizer = optim.Adam(model.parameters(), lr=adam_lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # split data\n",
    "        train_loader = DataLoader(\n",
    "            Subset(data_list, train_idx),\n",
    "            shuffle=True,\n",
    "            batch_size=32,\n",
    "        )\n",
    "        valid_loader = DataLoader(\n",
    "            Subset(data_list, valid_idx),\n",
    "            shuffle=False,\n",
    "            batch_size=32,\n",
    "        )\n",
    "\n",
    "        for epoch_idx in range(num_epoch):\n",
    "            # train\n",
    "            cv_train(model, train_loader, device, criterion, optimizer, model_name)\n",
    "            # valid\n",
    "            valid_acc = cv_test(model, valid_loader, device, model_name)\n",
    "\n",
    "        valid_accs.append(valid_acc)\n",
    "\n",
    "    return valid_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac6217a-25f7-4d42-8985-090321552c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TUDataset(root='data/TUDataset', name=\"COLLAB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ef1e78e-79c5-49d3-af26-cc6c3eaef281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a58e07-7362-4c85-8603-a1370e2174c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [00:00<00:00, 20067.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTAG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n",
      "0.77\n",
      "0.7\n",
      "0.67\n",
      "0.86\n",
      "0.67\n",
      "REDDIT-BINARY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 515.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.64\n",
      "0.55\n",
      "0.57\n",
      "0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1178/1178 [00:00<00:00, 24523.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n",
      "DD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68\n",
      "0.74\n",
      "0.59\n",
      "0.6\n",
      "0.61\n",
      "0.62\n",
      "COLLAB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:44<00:00, 112.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n",
      "0.54\n",
      "0.62\n",
      "0.57\n",
      "0.54\n",
      "0.67\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"model\", \"name\", \"aggre\", \"adam_lr\", \"ave_acc\", \"std_acc\"])\n",
    "\n",
    "cnt = 0\n",
    "for dataset_name in [\"MUTAG\", \"REDDIT-BINARY\", \"DD\", \"COLLAB\"]:\n",
    "    print(dataset_name)\n",
    "    dataset = TUDataset(root='data/TUDataset', name=dataset_name)\n",
    "    \n",
    "    torch.manual_seed(12345)\n",
    "    dataset = dataset.shuffle()\n",
    "    \n",
    "    # node attributeがない場合 degreeをnode attribute として使用\n",
    "    if dataset[0].x is None:\n",
    "        for d in dataset:\n",
    "            d.x = make_x(d)\n",
    "    \n",
    "    datalist = []\n",
    "    for d in tqdm(dataset):\n",
    "        if d.x is None:\n",
    "            d.x = make_x(d)\n",
    "        datalist.append(d)\n",
    "    \n",
    "    if dataset.num_node_features == 0:\n",
    "        feature = 1\n",
    "    else:\n",
    "        feature = dataset.num_node_features\n",
    "    \n",
    "    \n",
    "    for model_name in [\"GCN\", \"DGCNN\"]:\n",
    "            for adam_lr in [0.01, 0.0001]:\n",
    "                if model_name == \"GCN\":\n",
    "                    for p in [\"mean\", \"sum\"]:\n",
    "                        valid_accs = CV(datalist, 10, dataset.num_classes,\n",
    "                                        feature, model_name, adam_lr, pool=p)\n",
    "\n",
    "                        info  = {\"model\": model_name,  \"name\": dataset_name, \"aggre\": p,\n",
    "                         \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "                        }\n",
    "\n",
    "                        df = df.append(pd.Series(info, name=cnt))\n",
    "                        cnt += 1\n",
    "                        print(round(np.average(valid_accs),2))\n",
    "                else:\n",
    "                    valid_accs = CV(datalist, 10, dataset.num_classes, \n",
    "                                    feature, model_name, adam_lr)\n",
    "\n",
    "                    info  = {\"model\": model_name, \"name\": dataset_name, \"aggre\": \"None\",\n",
    "                     \"adam_lr\": adam_lr, \"ave_acc\": np.average(valid_accs), \"std_acc\": np.std(valid_accs)\n",
    "                    }\n",
    "\n",
    "                    df = df.append(pd.Series(info, name=cnt))\n",
    "                    cnt += 1\n",
    "                    print(round(np.average(valid_accs),2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f3eb2cd-823b-41b2-92bd-048000d6e339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>name</th>\n",
       "      <th>aggre</th>\n",
       "      <th>adam_lr</th>\n",
       "      <th>ave_acc</th>\n",
       "      <th>std_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.744444</td>\n",
       "      <td>0.085149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.765497</td>\n",
       "      <td>0.106818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GCN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.696199</td>\n",
       "      <td>0.109799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GCN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.665497</td>\n",
       "      <td>0.128590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DGCNN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.855848</td>\n",
       "      <td>0.065094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DGCNN</td>\n",
       "      <td>MUTAG</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.665497</td>\n",
       "      <td>0.128590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model   name aggre  adam_lr   ave_acc   std_acc\n",
       "0    GCN  MUTAG  mean   0.0100  0.744444  0.085149\n",
       "1    GCN  MUTAG   sum   0.0100  0.765497  0.106818\n",
       "2    GCN  MUTAG  mean   0.0001  0.696199  0.109799\n",
       "3    GCN  MUTAG   sum   0.0001  0.665497  0.128590\n",
       "4  DGCNN  MUTAG  None   0.0100  0.855848  0.065094\n",
       "5  DGCNN  MUTAG  None   0.0001  0.665497  0.128590"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"paper_result/re_benchmark_CV.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
